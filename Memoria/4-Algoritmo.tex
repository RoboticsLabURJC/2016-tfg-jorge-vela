\chapter{Algoritmo de Navegaci\'on aut\'onoma}\label{cap.algoritmo} %Algoritmo de Navegaci\'on aut\'onoma

\hspace{1 cm} En este cap\'itulo se describe el modo por el cual, con la infraestructura que se tiene, se llega a una soluci\'on para lo planteado en los objetivos. 

\hspace{1 cm} Este algoritmo tiene que permitir al drone despegar de forma controlada, realizar una navegaci\'on aut\'onoma para encontrar una baliza sobre la que aterrizar. La organizaci\'on de este cap\'itulo tiene primero una secci\'on de diseño, en el que se explica el funcionamiento del programa. Tras esto una secci\'on de percepci\'on para explicar los datos que se obtienen a trav\'es de los sensores, y para finalizar una secci\'on de control, para explicar los distintos movimientos del drone y de que dependen estos. %añadir algoritmo de control

\section{Diseño}
\label{sec.Design}

\hspace{1 cm} El diseño de este algoritmo consta de un comportamiento reactivo de iteraciones continuas. Es un proceso basado en adquisici\'on-procesado-env\'io de datos. La adquisici\'on de los datos se realizan mediante los sensores del drone. Estos datos ser\'an transmitidos al dispositivo para que lo procese y tras estos se enviar\'an las instrucciones al drone para que este las ejecute. El sensor utilizado en este proyecto ha sido la c\'amara, y los movimientos del drone depender\'an de lo que esta capte en cada momento. 

\hspace{1 cm} Por otro lado, la parte es un aut\'omata finito de estados, el cual comienza en un estado inicial (despegue), y en funci\'on de lo que recibe a la entrada (imagen del sensor), realizar\'a el procesamiento necesario para producir la informaci\'on que enviar al drone, y en ocasiones le llevar\'a a pasar de un estado a otro. Destacar que este aut\'omata est\'a compuesto por 6 estados: despegando, buscando, posible baliza, centr\'andose en la baliza, aterrizando y aterrizado. A excepci\'on del primero, los dem\'as estados depender\'an de lo que detecte el drone en cada momento. El primer estado est\'a controlado por tiempo, pues son 10 segundos al iniciarse en los que el drone inicia el vuelo sobre una baliza y trata de estar centrado sobre \'esta, para as\'i tener un despegue controlado y evitar que el drone se mueva en caso de tener alguna deriva o haya factores externos que produzcan esto. Una vez transcurridos los 10 segundos se pasar\'a al estado de b\'usqueda e ir\'a pasando por los distintos estados hasta su aterrizaje. 

\begin{figure}[H]
	\centering
		\includegraphics[width=0.75\textwidth]{imgs/esquema2.png}
	\label{fig:esquema_d}
\end{figure}

\hspace{1 cm} El lugar de aterrizaje del drone es una baliza previamente definida. Esta baliza es un cuadrado que en su interior tiene cuatro cuadrados, dos naranjas, y dos azules o verdes, dependiendo si se trabaja con el drone real o con el simulador. Esta baliza se ha elegido para que sea dif\'icil confundirla con otro objeto, pues de ser una baliza simple se podr\'ian confundir los colores. De esta forma lo que buscaremos ser\'a la cruceta que forman estos cuatro cuadrados y el punto central de \'esta. Por lo tanto, en lugar de un objeto en si lo que se busca es un objeto que tenga determinadas caracter\'isticas y patrones, algo que se puede detectar a diferentes alturas, distintas condiciones y situaciones.

\begin{figure}[H]
	\centering
		\includegraphics[width=0.3\textwidth]{imgs/baliza.jpg}
         \caption{Baliza utilizada para el drone real.}
	\label{fig:esquema_d}
\end{figure}


\hspace{1 cm} Por \'ultimo, para el control del drone con nuestro algoritmo se ha utilizado la herramienta follow\_turtlebot de JdeRobot Academy, explicada en \ref{sec.JdeRobotAcademy} . \'Esta aplicaci\'on tiene un interfaz gr\'afico de usuario (GUI) que permite controlar el drone y ver los datos de los distintos sensores, as\'i como la imagen que obtiene la c\'amara. Por otro lado cuenta con las interfaces ICE que permite la comunicaci\'on con el servidor, pudiendo as\'i recibir datos de sensores y motores, y enviar las instrucciones de velocidad necesarias en cada momento. La imagen de la aplicaci\'on se encuentra en la figura \ref{fig:FollowTurtlebot}


\section{Percepci\'on}

\hspace{1 cm} En \'esta secci\'on se va a tratar la obtenci\'on de la imagen de la c\'amara y el procesamiento que se realiza sobre \'esta. Gracias a lo que el drone ve en todo momento, sabe el punto del proceso en el que se encuentra y la informaci\'on que debe enviar. En primer lugar, se obtiene una imagen de entrada. Esta imagen es procesada con filtros de color y operadores morfol\'ogicos, obteniendo una imagen de salida. A partir de los datos de \'esta imagen, se detecta si hay objetos de inter\'es o no, y por tanto se env\'ia unas instrucciones u otras al drone. 


\subsection{Pre-procesado}

\hspace{1 cm} Una vez tenemos la imagen de entrada, hay que detectar la informaci\'on de inter\'es que nos aporta. 
 
\hspace{1 cm} En primer lugar, la imagen que est\'a en RGB se transforma a HSV, para que sea m\'as facil su interpretaci\'on, ya que en lugar de trabajar en funci\'on de tres colores (rojo, verde y azul), se trabaja en funci\'on de tres par\'ametros (tono, saturaci\'on y valor). De \'esta forma, se depende menos de la luz que haya en cada momento y en cada lugar. En el filtro, a cada uno de los par\'ametros se le asigna un rango de valores entre 0 y 255. La herramienta colorTuner permite, a partir de una imagen de entrada, dar valores a estos par\'ametros, viendo que objetos cumplen estos requisitos y dejandolos en primer plano, y cuales no, dejando estas zonas en negro como p\'ixeles de fondo. Una vez se obtienen estos valores se añaden al filtro, obteniendo a la salida una imagen que solo muestra los objetos de los colores de inter\'es. 

\hspace{1 cm} El siguiente c\'odigo es un ejemplo de como, a partir de una imagen de entrada en RGB, se transforma a HSV y se filtran los objetos de color naranja. 

\begin{lstlisting}[backgroundcolor=\color{yellow}]
hsv = cv2.cvtColor(input_image, cv2.COLOR_BGR2HSV)
lower_orange = np.array([100,100,80], dtype=np.uint8)
upper_orange = np.array([150, 255,255], dtype=np.uint8)
maskOrange = cv2.inRange(hsv, lower_orange, upper_orange)
maskRGBOrange = cv2.bitwise_and(input_image,input_image, mask= maskOrange)
\end{lstlisting}


\hspace{1 cm} En caso de trabajar sobre simulador, con esto se obtiene a la salida una imagen bastante parecida a la deseada, debido a la pureza e los colores. Sin embargo, al trabajar sobre im\'agenes reales, la imagen de salida de este filtro a\'un tiene ruido y objetos de inter\'es imperfectos. Para arreglar esto se utilizan los operadores morfol\'ogicos, que eliminan estas imperfecciones. Una breve explicaci\'on de los utilizados es la siguiente: 

\begin{itemize}
	\item \textbf{Erosi\'on:} Dada una imagen y un elemento estructural, la erosi\'on es el conjunto de los elementos \textit{x} para los cuales el elemento estructural trasladado por \textit{x} est\'a contenido en la imagen. 
	\newline\hspace{1 cm} Aplicaci\'on: Cuando un p\'ixel que parece pasar el filtro, pero los elementos de su alrededor (en concordancia con el elemento estructurante) no lo pasan, \'este pasa a ser parte del fondo. 
	\item \textbf{Dilataci\'on:} Transformaci\'on dual a la erosi\'on. El resultado de \'esta es el conjunto de elementos tal que al menos alg\'un elemento del conjunto estructurante esta contenido en \textit{x}, cuando el elemento estructurante se desplaza sobre \textit{x}
	\newline\hspace{1 cm} Aplicaci\'on: p\'ixeles que parecen de fondo, pasan a ser de la figura si est\'an cerca de p\'ixeles que pasan el filtro.
	\item \textbf{Cierre:} Se trata de realizar una dilataci\'on en la imagen seguida de una erosi\'on.
	\item \textbf{Apertura:} Se trata de realizar la erosi\'on en una imagen seguida de una dilataci\'on.
\end{itemize}


\hspace{1 cm} De esta forma, al obtener una imagen sin apenas ruido, evitando as\'i que objetos de no inter\'es los detecte como tal y que los objetos de inter\'es los trate como p\'ixeles de fondo. Una im\'agen en la que se observa la realizaci\'on de un filtro de color y el uso de operadores morfol\'ogicos es la siguiente:

\begin{figure}[ht]
	\centering
		\includegraphics[width=0.8\textwidth]{imgs/colorfilter.eps}
	\label{fig:E_Imagen_baliza}
\end{figure}


\hspace{1 cm} El siguiente fragmento de c\'odigo, muestra como se define la matriz a partir de la cual se realizar\'an las operaciones oportunas, y despues se realizan la erosi\'on y la dilataci\'on.
\begin{lstlisting}[backgroundcolor=\color{yellow}]
kernel = np.ones((3,3),np.uint8)
maskRGBOrange = cv2.erode(maskRGBOrange,kernel,iterations = 4)
maskRGBOrange = cv2.dilate(maskRGBOrange,kernel,iterations = 3)
\end{lstlisting}


\subsection{Detecci\'on de la baliza}
\hspace{1 cm} Una vez se obtienen los colores de los objetos de inter\'es, hay que buscar el objeto deseado. Para \'esto hay que buscar el punto de intersecci\'on entre los cuatro cuadrados de la baliza. Para ello, los pasos a realizar son los siguientes:

\begin{enumerate}
	\item Para la imagen de entrada se hacen dos filtros de color, uno por cada color de la baliza.
	\item De cada imagen se obtienen el n\'umero de objetos, las \'areas y los contornos. En cada objeto se obtiene el valor de su \'area, en caso de ser menor de un valor determinado, \'este se descarta. En caso de tener ese \'area o mayor, se crea una imagen negra, se pintan los contornos del objeto sobre esta con un valor RGB(0,1,0) y se dilatan. De esta forma, tendremos tantas im\'agenes como objetos. 
	\item Se suman las im\'agenes obtenidas, por lo tanto, sobre una imagen negra final se pintaran los contornos dilatados de todos los objetos. Cada contorno se pintar\'a con un valor RGB(0,1,0), por lo tanto en el caso de que varios objetos coincidan sumar\'an sus valores. Con esto conseguiremos que el punto donde interseccionen cuatro objetos tenga un valor (0,4,0), y por tanto \'este ser\'a el centro de la cruceta.
	
\hspace{1 cm}El siguiente fragmento de c\'odigo muestra la creaci\'on de una imagen por objeto, la dilataci\'on de los contornos y la suma final de las im\'agenes:

\begin{lstlisting}[backgroundcolor=\color{yellow}]
f = []
i=0
imgray2 = cv2.cvtColor(maskRGBOrange,cv2.COLOR_BGR2GRAY)
ret,thresh = cv2.threshold(imgray2,255,255,255)
_,contours, hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,
                                          cv2.CHAIN_APPROX_SIMPLE)
\'areas = [cv2.contour\'area(c) for c in contours]
for extension in \'areas:
    if extension > 100:
    img = np.zeros((y_img*2,x_img*2,3), np.uint8)
        actual = contours[i]
        approx = cv2.approxPolyDP(actual,0.05*cv2.arcLength(actual,True),
                                                                       True)
        cv2.drawContours(img,[actual],0,(0,30,0),12)
        f.append(img)
        i=i+1
			
kernel = np.ones((3,3),np.uint8)
if(len(f)>0):
    f[0] = cv2.dilate(f[0],kernel,iterations = 4)
    show_image2=f[0]
    for k in range(len(f)-1):
        f[k+1] = cv2.dilate(f[k+1],kernel,iterations = 4)
        show_image2=show_image2+f[k+1]
		
\end{lstlisting}


	\item A partir de la imagen final, se pasar\'a un filtro RGB que filtre los valores mayores a RGB(0,3,0), por lo tanto el \'unico valor que no se eliminar\'a sera el del centro de la cruceta.

	\item Sobre la imagen obtenida, utilizando la funci\'on drawcontours, obtenemos la situaci\'on de estos y p\'ixeles, teniendo as\'i la situaci\'on de la cruceta, y con estos valores se puede marcar el centro de la baliza sobre la imagen real. En caso de haber filtrado la intersecci\'on de dos objetos en lugar de cuatro en el apartado anterior, se podr\'ian marcar tambi\'en posibles objetos de inter\'es, como puede ser la baliza completa. 

\hspace{1 cm}En el siguiente fragmento de c\'odigo se muestra, como a partir de la imagen de la suma de objetos, se calcula la posici\'on de la cruceta y se marcan sobre la imagen. En \'este caso, para que se obtuviera una imagen m\'as clara, a los bordes de los objetos se les da un valor de 30, por lo tanto la intersecci\'on de 3 objetos tendr\'a un valor RGB(0,90,0) y de 4 objetos un valor RGB(0,120,0).

\begin{lstlisting}[backgroundcolor=\color{yellow}]
lower_green = np.array([0,80,0], dtype=np.uint8) 
upper_green = np.array([0, 140,0], dtype=np.uint8) 
maskSHI = cv2.inRange(show_image2, lower_green, upper_green)
show_image2 = cv2.bitwise_and(show_image2,show_image2, mask= maskSHI)

compare_image = np.zeros((y_img*2,x_img*2,3), np.uint8)
diff_total = cv2.absdiff(compare_image, show_image2)

imagen_gris = cv2.cvtColor(diff_total, cv2.COLOR_BGR2GRAY)
_,contours,_ = cv2.findContours(imagen_gris,cv2.RETR_EXTERNAL, 
                                             cv2.CHAIN_APPROX_SIMPLE)

positionX=-1
positionY=-1
for c in contours:
    if(cv2.contour\'area(c) >= 0):
        posicion_x,posicion_y,ancho,alto = cv2.boundingRect(c) 
        cv2.rectangle(show_image,(posicion_x,posicion_y),
                    (posicion_x+ancho,posicion_y+alto),(0,0,255),2)
        positionX= (posicion_x+posicion_x+ancho)/2
        positionY= (posicion_y+posicion_y+ancho)/2
\end{lstlisting}

\end{enumerate}

\hspace{1 cm} En el conjunto de im\'agenes siguiente, se observa la imagen de entrada, en la cual esta marcado el centro de la baliza y el objeto de inter\'es, y la suma de los bordes de los distintos objetos, siendo los puntos donde m\'as contornos se cruzan de un color m\'as intenso. 

\begin{figure}[H]
 \centering
  \subfloat[Imagen real]{
   \label{f:imagen real}
    \includegraphics[width=0.29\textwidth]{imgs/k_beacon21.png}}
  \subfloat[Suma de objetos]{
   \label{f:sumaobjetos}
    \includegraphics[width=0.33\textwidth]{imgs/k_Beacon1.eps}} 
 \caption{Procesamiento de imagen de la baliza}
 \label{f:ColorFilterTotal}
\end{figure} 



\section{Control}
\label{sec.control}

\hspace{1 cm} En \'esta secci\'on se explica el control sobre el drone en funci\'on del momento en el que se encuentra y la imagen que se obtiene. Se puede dividir en tres partes:
despegue, b\'usqueda y aterrizaje.


\subsection{Despegue}

\hspace{1 cm} Esta fase est\'a controlada por tiempo. Son los primeros diez segundos del algoritmo, y en ellos el drone despega de forma controlada. Esta fase ser\'a "`Take off"' en el aut\'omata de estados. Para ello se sit\'ua el drone sobre una baliza sobre la cual tiene que estabilizarse. De esta forma, al despegar detecta \'esta y trata de centrarse, evitando as\'i que se desv\'ie por factores externos y qued\'andose en la situaci\'on correcta. Debido a que el drone va a despegar sobre la baliza, el control en esta parte es un control proporcional, para evitar que tenga que hacer m\'ultiples operaciones, por tanto sea mas r\'apido el algoritmo, y suficiente para ser controlado.



\subsection{B\'usqueda}
\hspace{1 cm} Esta fase comenzar\'a cuando finalicen los diez segundos de despegue. El drone comenzar\'a a navegar de forma aut\'onoma en b\'usqueda de una baliza sobre la cual aterrizar. Para ello comenzar\'a un algoritmo en espiral, estado "`Search"', de forma que ir\'a rastreando la zona ampliando su giro de forma continua, hasta que detecte una posible baliza. En el momento que la detecte, pasar\'a al estado "`Possibly beacon"' e intentar\'a centrarse sobre ella. Si tras un n\'umero previamente definido de iteraciones, no se trata de una b\'aliza, continuar\'a el algoritmo de b\'usqueda en el punto donde se hab\'ia quedado, siguiendo con la amplitud de las espirales a la que se hab\'ia llegado. Pero en caso de serlo pasar\'a al estado "`Centering beacon"', haciendo que coincida el centro de la baliza con el centro de la imagen de la c\'amara. Para realizar el movimiento de centrarse en la baliza se ha utilizado un control PD (progresivo y derivativo). Esto se debe a que s\'olo con el control progresivo, se produc\'ia una gran diferencia de velocidad y el drone cambiaba sus giros de forma muy brusca, lo que le llevaba a desestabilizarse y perder con facilidad la baliza. El algoritmo del control PD es de la siguiente forma:

\begin{itemize}
\item Por una parte, se tiene el control progresivo. Para \'este se obtiene por un lado el centro de la imagen, y por otro lado el centro de la cruceta o del objeto de interes, seg\'un el punto del algoritmo. Se calcula la diferencia entre el centro de la imagen y el centro de referencia, obteniendo as\'i el error, y como resultados, para el eje x $\Delta_x$,  y para el eje y $\Delta_y$. Estos valores, se multiplican por una constante que sirve para adaptar el resultado a la velocidad del drone. La formula de este control es la siguiente:  \[\Delta_x = centroimagen_x - centroobjeto_x\]   \[\Delta_y = centroimagen_y - centroobjeto_y\]  \[ P_x = \Delta_x * 0.01\]  \[P_y = \Delta_y * 0.01\]


\item Por otro lado, est\'a el control derivativo. Para este caso, al tratarse de iteraciones, se trabaja con las diferentes muestras de cada iteración. Se obtiene el error de la iteraci\'on actual y de la iteraci\'on anterior para ambos ejes, y se resta el error anterior al actual: \[\Delta_d_i_f_e_r_e_n_c_i_a_x =  \Delta_x [n] - \Delta_x [n-1] \] \[\Delta_d_i_f_e_r_e_n_c_i_a_y =  \Delta_y [n] - \Delta_y [n-1] \]

\end{itemize}

\hspace{1 cm} Una vez se ha obtenido esto, se env\'ian las instrucciones de velocidad al drone, utilizando el comando:
\begin{lstlisting}[backgroundcolor=\color{yellow}]
vely = (y_img-positionY)                        
velx = (x_img-positionX)

Kp=0.01
vy_P = vely*Kp 
vx_P = velx*Kp

Kd = 0.003
vx_D = abs(xanterior-velx)*Kd
vy_D = abs(yanterior-vely)*Kd

Vy_tot = vy_P + vy_D
Vx_tot = vx_P + vy_D
self.cmdvel.sendCMDVel(Vy_tot,Vx_tot,0,0,0,0) 

xanterior = velx
yanterior = vely

if(abs(Vx_tot-xanteriorTot)>0.3):
   if(Vx_tot<xanteriorTot):
      Vx_tot = xanteriorTot-0.3
   else:
      Vx_tot = xanteriorTot+0.3                                 

if(abs(Vy_tot-yanteriorTot)>0.3):
   if(Vy_tot<yanteriorTot):
      Vy_tot = yanteriorTot-0.3
   else:
      Vy_tot = yanteriorTot+0.3                                 
yanteriorTot = vytot
xanteriorTot = vxtot

self.cmdvel.sendCMDVel(Vy_tot,Vx_tot,0,0,0,0) 
\end{lstlisting}
	

 %$\\ Calculo \Delta_x : \[\Delta_x = centroimagen_x - centroobjeto_x\] \\ Calculo \Delta_x : \[\Delta_y = centroimagen_y - centroobjeto_y\] \\ Calculo del control proporcional P_x : \[ P_x = \Delta_x * 0.01\] \\ Calculo del control proporcional P_y : \[P_y = \Delta_y * 0.01\] 




%\hspace{1 cm} Esta fase comenzar\'a cuando finalicen los diez segundos de despegue. El drone comenzar\'a a navegar de forma aut\'onoma en b\'usqueda de una baliza sobre la cual aterrizar. Para ello comenzar\'a un algoritmo en espiral, estado "`Search"', de forma que ir\'a rastreando la zona ampliando su giro de forma continua, hasta que detecte una posible baliza. En el momento que la detecte, pasar\'a al estado "`Possibly beacon"' e intentar\'a centrarse sobre ella. En caso de no tratarse de la baliza, continuar\'a con el algoritmo de b\'usqueda, pero en caso de serlo pasar\'a al estado "`Centering beacon"', haciendo que coincida el centro de la baliza con el centro de la imagen de la c\'amara. Para realizar el movimiento de centrarse en la baliza se ha utilizado un control PID (progresivo, integral y derivativo). Esto se debe a que s\'olo con el control progresivo, se produc\'ia una gran diferencia de velocidad y el drone cambiaba sus giros de forma muy brusca, lo que le llevaba a desestabilizarse y perder con facilidad la baliza.  Este control se basa en derivar el error con respecto al tiempo y multiplicarlo por una constante. Dado que nuestro algoritmo ejecuta una vez cada cierto tiempo y no esta continuamente pasando por este punto, podemos considerar que se trata de un sistema en tiempo discreto, y por tanto en lugar de trabajar con derivadas trabajaremos con sumatorios. De esta forma, para añadir un control derivativo se realiza una operaci\'on que depende de la velocidad anterior y la que tenemos ahora: \[v_{derivativa} =1-(v_{anterior}-v_{nueva})/50 \] 

%\hspace{1cm}Para evitar que el valor sea 0 y al multiplicarlo por el valor progresivo se quede en el sitio, en caso de ser un valor menor a 0.1 se iguala a \'este. Este resultado lo multiplicamos a la velocidad final, y lo que conseguimos es que si el valor entre dos velocidades continuas es muy alto, \'este se aten\'ue de forma que el drone no cambie mucho y vaya oscilando, sino que lleve una velocidad m\'as continua a la hora de acelerar o frenar.  

El siguiente fragmento de c\'odigo consigue que el drone se mueva realizando espirales: \\

\begin{lstlisting}[backgroundcolor=\color{yellow}]
self.cmdvel.sendCMDVel(1.8+wSearch,0,0,0,0,1.5 - wSearch)
numVuelta=numVuelta+1
if(numVuelta==100):
    timerW=timerW+(timerW/8)
    numVuelta=0
    if(wSearch<1):
        wSearch=wSearch+0.2
\end{lstlisting}

\hspace{1 cm} El esquema de este control cuando el drone est\'a en proceso de b\'usqueda y detecta una imagen es el siguiente:
\begin{figure}[ht]
	\centering
		\includegraphics[width=1\textwidth]{imgs/esquemapd2.png}
	\label{fig:Esquema_control}
\end{figure}


\hspace{1cm} Para visualizar el diagrama de estados, se tuvieron que añadir dos paquetes del GUI de JdeRobot, uno que permit\'ia abrir otra ventana y otro que permit\'ia añadir estados y transiciones entre ellos. Para marcarlo, siendo el numero que aparece el numero del estado que queremos marcar, val\'ia con la siguiente linea de c\'odigo:

\begin{lstlisting}[backgroundcolor=\color{yellow}]
self.machine.setStateActive(2, True)
\end{lstlisting}
	
\hspace{1cm}La imagen del diagrama que ver\'iamos durante la ejecuci\'on se encuentra en \ref{fig:Diag_estados}.

\subsection{Aterrizaje}

\hspace{1 cm} Esta fase es un aterrizaje controlado. Una vez el drone se ha centrado sobre la baliza comienza a descender de forma constante , cambiando el estado a "`Landing"'. Una vez el \'area es menor a 19272135.0, se manda la instrucci\'on de parar motores. En este momento el drone desciende hasta posarse en el suelo, entonces para los motores y el estado cambia a "`Landed"'. La raz\'on de ir descendiendo poco a poco hasta detectar determinado \'area, es por si la baliza est\'a lejos del drone, puede interferir alg\'un objeto moment\'aneamente o darse alg\'un factor externo que altere la posici\'on del drone respecto de la baliza, y al enviar la instrucci\'on de aterrizar se perder\'ia este control. El \'area elegido es porque en ese momento la baliza ocupa pr\'acticamente por completo la imagen, y si se le indicara al drone que siguiera descendiendo las corrientes que se producir\'ian con el suelo podr\'ian afectarle y desviar su trayector\'ia, pudiendo perder la referencia.


\hspace{1 cm} Una vez finalizada la descripci\'on del algoritmo completo, un esquema del control completo es el siguiente:
\begin{figure}[H]
	\centering
		\includegraphics[width=1\textwidth]{imgs/EsquemaAlgoritmo.jpg}
	\label{fig:Esquema_control}
\end{figure}


%Esta fase es un aterrizaje controlado, en el cual, una vez que el drone se ha centrado sobre la baliza comienza a descender, cambiando su estado a "`Landing"', y una vez el \'area de la baliza es suficiente, como para detectar que el drone est\'a muy pr\'oximo a ella, se env\'ia la instrucci\'on land, aterrizando de \'esta forma el drone, siendo aqu\'i su estado "`Landed"'. El hecho de enviarse la orden de aterrizar cuando se est\'a a una distancia cercana a la baliza es porque esta instrucci\'on es a ciegas, una vez que se env\'ia el drone solo se encarga de descender hasta que nota que est\'a sobre un lugar sobre el cual posarse. Por lo tanto si se env\'ia esta orden a una distancia lejana, el drone puede perder la referencia y aterrizar en otro sitio. 



\section{Arquitectura software}

\hspace{1 cm} El programa tiene que funcionar con fluidez, permitiendo enviar y recibir datos a la vez que se visualizan los datos que ya se tienen. Para ello, la aplicaci\'on follow\_turtlebot tiene una estructura que sigue el siguiente esquema:
%ENVIAR DATOS Y RECIBIR ORDENES

\hspace{1 cm} Primero est\'a el programa principal follow\_turtleboot.py. \'Este es el encargado de crear la m\'aquina de estados, decir el n\'umero de estados que este va a tener y las posibles transiciones entre unos y otros. Tambi\'en se encarga de definir las distintas herramientas de configuraci\'on que vaya a utilizar el algoritmo(c\'amara, datos de navegaci\'on, datos de posici\'on, comandos de velocidad y comandos extra para despegar y aterrizar) para pas\'arselos como par\'ametro a este. Una vez se tenga todo definido, se pasa a la creaci\'on de los distintos hilos.

\hspace{1 cm} Por un lado se crea el interfaz gr\'afica y la ventana en la que se muestra el diagrama de estados. Se hace que \'este se ejecute en un hilo y se le manda comenzar.

\hspace{1 cm} Por otro lado, se crea el interfaz de control, el cual crea una ventana que permite el control del drone y muestra la informaci\'on de sus sensores. A este se le pasan como datos las distintas herramientas anteriormente definidas para que pueda utilizarlas, como son la c\'amara para visualizar las imagenes o el cmdvel para enviar las instrucciones de velocidad. Esta ventana tambi\'en tiene los botones de "Play" y "Stop", los cuales permiten que ejecute el algoritmo creado o que pare su ejecuci\'on.

\hspace{1 cm} Para finalizar, se llama a la funci\'on que ejecutar\'a el algoritmo principal. A esta funci\'on se le pasan como parametros todo lo hasta ahora creado, para que as\'i pueda acceder a los datos de la c\'amara, los distintos sensores y el diagrama de estados. Esta funci\'on permitir\'a obtener los datos deseados de los sensores, enviar instrucciones a los motores y al diagrama de estados. Este algoritmo, adem\'as de tener la funci\'on principal y las creadas para el funcionamiento de este, tiene cuatro funciones esenciales para su funcionamiento. Por un lado est\'an las de "play" y "stop", las cuales mandan el evento de iniciar (en caso de la funci\'on Play), de parar (en caso de la funci\'on Stop), que se ejecutaran principalmente cuando se pulsen los botones del interfaz de control. Por otro lado est\'a la funci\'on "kill"", que se encarga de matar el proceso. Por ultimo est\'a la funci\'on "run", que es la encargada de llamar de forma continua al algoritmo a ejecutar mientras no se mate a la aplicaci\'on. Esta funci\'on, mientras se haya mandado la instrucci\'on de iniciar y no la de parar, se encontrar\'a en un bucle continuo obteniendo a final de cada iteraci\'on el tiempo que ha tardado en ejecutarse, en caso de no llegar a un m\'inimo de tiempo, la funci\'on parara la cantidad de tiempo necesaria para llegar a ese m\'inimo, y entonces dejar\'a continuar con la ejecuci\'on. De esta forma, la velocidad del algoritmo no solo depender\'a del algoritmo y las funciones en si, sino que en caso de tardar muy poco tendr\'a determinados tiempos de espera.

















